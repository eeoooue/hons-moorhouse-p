
#### Currently

Constraints: "None"
Assumptions: "That Structural Benchmarking is an indicator of general performance at MSA"

#### Could Change To:

Constraints: "Needing to work with an established bioinformatics file format for compatibility with assessment methods from the literature - as no bioinformaticians are in the project staff"

Assumptions: "That MSA software can be assessed by a non-expert using the methodologies outlined in the literature alone - i.e. scoring well in an evaluation is a sufficient indicator of good performance at MSA"


> Feels like a big change, but I only mention structural benchmarking ~5 times so maybe this is a relatively straightforward fix


There are 3 main methods for evaluating MSA software:

Structural benchmarks - use reference alignments constructed by experts
Simulated benchmarks - simulate the process of evolution to construct a reference alignment
Consensus-based - does your software agree with other software (that is thought to be good?)